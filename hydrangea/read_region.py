"""Provides one class, ReadRegion, to read in sub-regions of a snapshot."""

import h5py as h5
import numpy as np
import os
import time

from functools import reduce
from operator import mul
from pdb import set_trace

import hydrangea.hdf5 as hdf5
import hydrangea.crossref as xr
from hydrangea.split_file import SplitFile, get_subfind_index, get_fof_index
from hydrangea.reader_base import ReaderBase


class ReadRegion(ReaderBase):
    """
    Set up a region for efficient reading of data from snapshot files.

    This class can be called with several parameters to allow easy setup
    of commonly encountered selection regions (sphere, cube, box). Internally,
    the particle map generated by MapMaker is then read and processed into a
    (typically small) number of segments to read in.

    Once set up, any catalogue entry can be accessed as an attribute of
    the class instance, they are loaded when first encountered. As an
    alternative, entries can also be read explicitly with the
    :meth:`read_data` method.

    Parameters
    ----------
    file_name: string
        The path of the file containing the data to read. If the data
        is spread over multiple files, it can point to any one of them.
    part_type : int
        The particle type code to read (0=gas, 1=DM, 4=stars, 5=BHs)
    anchor : ndarray (3)
        The coordinates of the 'anchor point' of the selection region
        (its centre or corner, depending on `anchor_style`).
        Its units are specified by `coordinate_units` (see below).
    size : float or ndarray
        The extent of the selection region. The format of this
        parameter depends on the region `shape`: a single float
        for ``'sphere'`` or ``'cube'``, or a sequence of three values
        for ``'box'``. For a sphere, this specifies its radius, for a
        cube its half-side-length, and for a box the half-side-lengths
        in the x, y, and z dimension, respectively (note the
        different interpretations for cube and box if `anchor_style`
        is set to ``'bottom'``, as described below).

    shape : string, optional
        The shape of the region to read from. Valid options are
        ``'sphere'`` (default), ``'cube'``, or ``'box'``
        (all case-insensitive).
    anchor_style : string, optional
        Specifies the location of the anchor within the selection
        region. Default is ``'centre'`` (``'center'`` also accepted).
        The alternative, ``'bottom'``, places the anchor on the bottom
        x, y, z corner of the box or cube instead. This parameter has
        no effect for a sphere.
    verbose : int, optional
        Frequency of log messages (default: 1 ==> few)
    exact : bool, optional
        Only load particles lying exactly within the specified region,
        at extra cost (internally reads particle positions). If False
        (default), typically also some particles slightly outside the
        selected region are loaded.
    units : str, optional
        Specifies the unit system for both the anchor and size input,
        and any data read from the selected region. Options are

        - ``'data'`` : as on file
        - ``'clean'`` : as 'data', but with a and h factors removed
        - ``'astro'`` : 'astronomically sensible' units
          (e.g. pMpc, M_sun)
        - ``'si'`` : the standard SI unit system
        - ``'cgs'`` : alternative unit system favoured by astronomers

        Capitalization is ignored for all unit names.
    coordinate_units : str or None, optional
        Unit system in which to interpret the input coordinates.
        If None (default), the same as `units` is used. Capitalization
        is ignored.
    read_units : str or None, optional
        Unit system to which to convert read data (default: same as
        `units`). Capitalization is ignored.
    map_file : string, optional
        Location of the particle map file to use. If None (default),
        it is assumed to be ``'ParticleMap.hdf5'`` in the same directory
        as `file_name`.
    periodic : bool, optional
        Assume that the simulation volume is periodic and completely
        tiled with particle map cells (default: False). This option
        is not applicable to any of the Hydrangea/C-EAGLE simulations.
    load_full : bool, optional
        Load entire particle catalogue irrespective of specified region
        (default: False). If `exact` is True, the data will still
        be cut to the exact shape of the selected region afterwards.

    join_threshold : int, optional
        Threshold number of segments above which directly adjoining
        segments are joined to speed up the reading (default: 100).
    bridge_threshold : int, optional
        Threshold number of segments remaining after directly
        neighbouring ones are joined to perform a second join round.
        In this, elements separated by a small gap are also joined
        (speeding up the reading at the expense of reading slightly
        more particles). Default: 100.
    bridge_gap : float, optional
        Maximum size of gaps to bridge in second joining round.
        Two segments are joined if the ratio between their combined
        and bridged lengths are greater than `bridge_gap` (default: 0.5).

    Note
    ----
    Setting up very large regions (>~ 20 Mpc) is not very efficient.
    Therefore, when the code detects that > 1M cells would have to be
    checked and potentially loaded, the region setup is abandoned and the
    entire particle catalogue will be read in. Full reading is also enforced
    when the sub-selection contains more than 40% of the full catalogue.
    """

    def __init__(self, file_name, part_type, anchor, size, shape='sphere',
                 anchor_style='centre', verbose=1, exact=False,
                 units='astro', coordinate_units=None,
                 read_units=None, map_file=None, periodic=False,
                 load_full=False, join_threshold=100, bridge_threshold=100,
                 bridge_gap=0.5):

        stime = time.time()

        # Store 'simple' parameters in internal variables
        self.file_name = file_name
        self.pt_num = part_type
        self.base_group = "PartType{:d}" .format(part_type)
        self.coordinates = np.array([*anchor, size], dtype=float)
        self.verbose = verbose
        self.exact = exact
        self.load_full = load_full
        self.periodic = periodic
        self.anchor_style = anchor_style

        if read_units is None:
            read_units = units.lower()
        else:
            read_units = read_units.lower()
        if coordinate_units is None:
            coordinate_units = units.lower()
        else:
            coordinate_units = coordinate_units.lower()
        self.read_units = read_units

        # Deal with different shape inputs
        if shape is None:
            self.shape = 'sphere'
        elif shape.lower() in ['sphere', 'box', 'cube']:
            self.shape = shape.lower()
        else:
            print("Your shape '{:s}' is not understood. Sorry."
                  .format(shape))
            set_trace()

        # Set up map file, and check whether it exists
        if map_file is None:
            map_file = os.path.dirname(file_name) + '/ParticleMap.hdf5'
        if not os.path.exists(map_file):
            self._print(1, "Could not find particle map -- load everything.")
            self.load_full = True

        # Convert input coordinates to code, if necessary
        # N.B.: does not modify input array outside of function
        if coordinate_units != 'data':
            conv_input = self.get_unit_conversion('Coordinates',
                                                  coordinate_units)
            self.coordinates /= conv_input

        # If we need to load everything, we can quit now
        if self.load_full:
            self.num_particles = hdf5.read_attribute(
                file_name, 'Header', 'NumPart_Total')[part_type]
            if exact:
                self._find_exact_region()
            return

        # Do the actual work of finding segments to be loaded
        self._setup_region(map_file, join_threshold, bridge_threshold,
                           bridge_gap)

        # If we'd have to load too many cells, just load everything
        if self.load_full:
            self.num_particles = hdf5.read_attribute(
                file_name, 'Header', 'NumPart_Total')[part_type]
            if exact:
                self._find_exact_region()
            return

        self._print(1, "Region setup took {:.3f} sec."
                    .format(time.time()-stime))
        self._print(1, "Selection region contains {:d} cells, {:d} segments, "
                    "{:d} particles, {:d} files"
                    .format(self.num_cells, self.num_segments,
                            self.num_particles, len(np.unique(self.files))))
        self._print(2, "  (selected files:", np.unique(self.files), ")")
        if exact:
            self._print(1, f'Exact selection region contains '
                        f'{self.num_particles_exact} particles.')

        # If we'd have to load almost all particles, load everything
        if self.num_particles >= self.num_part_total * 0.75:
            self.load_full = True
            self._print(1, "Need to load {:.1f}% of total particle catalogue\n"
                        "  ==> faster to load everything instead"
                        .format(self.num_particles/self.num_part_total * 100))

    # -----------------------------------------------------------------------

    def read_data(self, dataset_name, units=None, verbose=None, exact=None,
                  file_name=None, pt_name=None, single_file=False,
                  store=False, trial=False, data_type=None):
        """Read a specified dataset within a previously set up region.

        Using this function provides an alternative to accessing data sets
        as attributes, with more flexibility on e.g. the units (for
        instance, data can be read in another unit system than what was
        specified as `read_units` during instantiation).

        Parameters
        ----------
        dataset_name : string
            The dataset to read from, including groups where appropriate.
            The leading ``'PartType[x]'`` must however *not* be included!

        units : str or None, optional
            Specifies the unit system for the output. Options are:

            - ``'data'`` : as on file
            - ``'clean'`` : as 'data', but with a and h factors removed
            - ``'astro'`` : 'astronomically sensible' units
              (e.g. pMpc, M_sun)
            - ``'si'`` : the standard SI unit system
            - ``'cgs'`` : alternative unit system favoured by astronomers

            If ``None`` (default), the class value `read_units` is used.
            Capitalization is ignored for all unit names.
        verbose : int, optional
            Frequency of log messages. If None (default), use class value.
        exact : bool, optional
            Only return data for particles lying in the exact specified
            selection region (default: class value).
        file_name : string, optional
            Specifies an alternative path to read data from. This is useful
            for reading data from ancillary catalogues. By default (None),
            the file name used to set up the class instance is used.
        pt_name : string, optional
            Specifies an alternative particle-type group name. By default
            (None), ``'PartType[x]'`` is used.
        single_file : bool, optional
            Assume that data resides in an un-split file (default: False).
            This is used only for ancillary catalogues.
        store : str or None or False, optional
            Store the retrieved array as an attribute with this name.
            If None, the (full) name of the data set is used, with
            '/' replaced by '__'. Default: False (do not store).
        trial : bool, optional
            Attempt to read the data set. If it does not yield the
            expected number of elements for any one file or total,
            return None. If False (default), enter debug mode in this case.
        data_type : str, optional
            Store read data in an array of this data type. If None
            (default), this is determined from the HDF5 data set.

        Returns
        -------
        data : array
            The data read for the particles in the selected region.

        Note
        ----
        The selection of which particles to load has already been done
        when the ReadRegion object was instantiated.
        """
        stime = time.time()

        # Set up default values for non-bool optional parameters
        if file_name is None:
            file_name = self.file_name
        if pt_name is None:
            pt_name = self.base_group
        if exact is None:
            exact = self.exact
        if verbose is None:
            verbose = self.verbose
        if exact and not self.exact:
            self._print(1, "Region not setup with exact loading. "
                        "Ignoring request for exact particle loading.")
            exact = False

        full_dataset_name = pt_name + '/' + dataset_name

        if units is None:
            units = self.read_units
        else:
            units = units.lower()

        # Deal with special case of loading the full particle catalogue
        if self.load_full:
            if single_file:
                data_full = hdf5.read_data(file_name, full_dataset_name)
                if units != 'data':
                    conv_fac = self.get_unit_conversion(dataset_name, units)
                    if conv_fac is not None and conv_fac != 1:
                        data_full *= conv_fac
            else:
                data_full = SplitFile(file_name, pt_name).read_data(
                    dataset_name, units=units)

            if exact:
                data_full = data_full[self.ind_sel, ...]

            # Store the array directly in the object, if desired
            if store is not False:
                if store is None:
                    store = dataset_name.replace('/', '__')
                setattr(self, store, data_full)

            self._print((1, verbose), "Reading '{:s}' took {:.3f} sec."
                        .format(dataset_name, time.time() - stime))
            return data_full

        # Deal with pathological case of no data to load:
        if self.num_segments == 0:
            return np.zeros(0, dtype=int)

        # -------------------------------------------------------------
        # Rest is for 'default' situation of reading via segment list.
        # -------------------------------------------------------------

        # Initialize current offset in output array
        write_offset = 0

        if single_file:
            f = h5.File(file_name, 'r')
        else:
            unique_files = np.unique(self.files)

            # To avoid duplicate work, open all files now and create a
            # reverse list to find the handles later
            rev_files = xr.ReverseList(unique_files)
            file_list = []
            for ifile in unique_files:
                curr_file_name = self._swap_file_name(file_name, ifile)
                file_list.append(h5.File(curr_file_name, 'r'))
            f = file_list[0]

        # Set up output before we iterate:
        try:
            dSet = f[full_dataset_name]
        except KeyError:
            if trial:
                return None
            print("Could not load data set, please investigate.")
            set_trace()

        full_shape = list(dSet.shape)      # Cannot assign to tuple ==> list
        full_shape[0] = self.num_particles
        if data_type is None:
            data_type = dSet.dtype
        data_full = np.empty(full_shape, data_type)

        # Get unit conversion factor
        conv_factor = self.get_unit_conversion(dataset_name, units)

        self._print((2, verbose), "Pre-reading '{:s}' took {:.3f} sec."
                    .format(dataset_name, time.time() - stime))

        # Read individual segments into correct location of output array
        for iiseg in range(self.num_segments):
            ifile = self.files[iiseg]
            self._print((2, verbose), "Segment {:d}, file {:d}"
                        .format(iiseg, ifile), end="")

            if not single_file:
                # Retrieve correct file handle
                handle_index = rev_files.query(ifile, assume_valid=True)
                if handle_index < 0:
                    print("Index inconsistency: {:d}..."
                          .format(handle_index))
                    set_trace()
                f = file_list[handle_index]

            # Load data set, checking that it actually exists
            try:
                dSet = f[full_dataset_name]
            except KeyError:
                print("Could not load data set, please investigate.")
                set_trace()

            # Find out from where and to where we should read
            read_offset = self.offsets[iiseg]
            read_end = self.offsets[iiseg] + self.lengths[iiseg]
            write_end = write_offset + self.lengths[iiseg]
            self._print((2, verbose), " [{:d} --> {:d}]"
                        .format(read_offset, read_end))

            # If we read from a single file, but the (main) particle data
            # are split, need to adjust location to read from in this file
            if single_file:
                ifile_offset = self.file_offsets[ifile]
                read_offset += ifile_offset
                read_end += ifile_offset

            if read_end-read_offset != write_end-write_offset:
                print("Inconsistency...")
                set_trace()

            # Actually read the data
            # (no appreciable speed difference between following two lines)
            #
            # data_full[writeOffset:writeEnd] = dSet[readOffset:readEnd]
            dSet.read_direct(data_full, np.s_[read_offset:read_end, ...],
                             np.s_[write_offset:write_end, ...])

            # Update write offset
            write_offset += self.lengths[iiseg]

        # Always a good idea to close files...
        if single_file:
            f.close()
        else:
            for f in file_list:
                f.close()

        # ------ Done with main bit, final adjustments if needed -----

        # Limit selection if 'exact' is set:
        if exact:
            data_full = data_full[self.ind_sel, ...]

        if conv_factor is not None and conv_factor != 1:
            data_full *= conv_factor

        # Store the array directly in the object, if desired
        if store is not False:
            if store is None:
                store = dataset_name.replace('/', '__')
            setattr(self, store, data_full)

        self._print((1, verbose), "Reading '{:s}' took {:.3f} sec."
                    .format(dataset_name, time.time() - stime))

        return data_full

    # `````````````````` End of read_data() ''''''''''''''''''''''''''''

    def total_in_region(self, dataset_name, average=False,
                        weight_quant=None, units='astro'):
        """
        Compute the total or average of a quantity [convenience function].

        Parameters
        ----------
        dataset_name : string
            The (full) name of the data set to process, including potential
            groups that contain it (but not the base group).
        average : bool, optional
            Compute the average of particles instead of the sum (default).
        weight_quant : string, optional
            The (full) name of a data set to use as weights. If None (default),
            no weighting is performed. Supplying a weight_quant implicitly
            also sets average=True. It is the user's responsibility to
            ensure that the weights do not sum to zero.
        units : str, optional
            Unit system to convert output to, as for :meth:`read_data`.

        Returns
        -------
        sum : array
            The sum or average over all particles in the selection region.
        """
        if self.exact is False:
            print("")
            print("**********************************************************")
            print("********************   WARNING ***************************")
            print("**********************************************************")
            print("")
            print("You have not set the 'exact' switch when establising this "
                  "region. Be aware that the reported total may include a "
                  "contribution from particles outside the target region. "
                  "Proceed with caution...")
            print("")
            print("**********************************************************")
            print("")

        data = self.read_data(dataset_name, units=units)

        if weight_quant is None:
            if average:
                return np.mean(data, axis=0)
            else:
                return np.sum(data, axis=0)
        else:
            weights = self.read_data(weight_quant, units=units)
            return np.average(data, weights=weights, axis=0)

    def in_subhalo(self, subhalo_index, subhalo_file=None):
        """Identify members of a subhalo within the reader."""
        if subhalo_file is None:
            subhalo_file = self.subhalo_file

        subhalo = SplitFile(subhalo_file, 'Subhalo', read_index=subhalo_index)
        ids = SplitFile(subhalo_file, 'IDs',
                        read_range=(subhalo.SubOffset,
                                    subhalo.SubOffset+subhalo.SubLength))
        ind_match, ind_matched = xr.find_id_indices(self.ParticleIDs,
                                                    ids.ParticleID)
        return ind_matched

    @property
    def SubhaloIndex(self):
        """Emulate a non-existing subhalo index for particles (attribute
        is computed on-the-fly when first accessed)."""
        if '_subhalo_index' not in dir(self):
            self._subhalo_index = get_subfind_index(self.ParticleIDs,
                                                    self.subfind_file)
        return self._subhalo_index

    @property
    def GroupIndex(self):
        """Emulate a non-existing group index for all particles (attribute
        is computed on-the-fly when first accessed)."""
        if '_group_index' not in dir(self):
            self._group_index = get_fof_index(self.ParticleIDs,
                                              self.subfind_file)
        return self._group_index

    # --------------------------------------------------------------
    # ----------- INTERNAL-ONLY FUNCTIONS BELOW --------------------
    # --------------------------------------------------------------

    def _setup_region(self, map_file, join_threshold=20,
                      bridge_threshold=20, bridge_gap_factor=0.5):
        """Set up the reading region with data from the particle map.

        The result is stored in the attributes
        num_cells, num_segments, num_particles, files, offsets, lengths

        Parameters
        ----------
        map_file : str
            The name of the particle map file.
        join_threshold : int or None
            Minimum number of segments for which simple join is attempted,
            fusing directly adjecent segments (default: 20). This speeds
            up the reading in exchange for a small increase in setup time.
            If None, no join is performed.
        bridge_threshold : int or None
            Minimum number of segments remaining after initial (simple)
            join to trigger a second join round in which also small gaps
            are bridged (default: 20). This speeds up the reading further,
            in exchange for a small increase in setup time and an increase
            in the number of read particles (and hence memory footprint).
        bridge_gap_factor : float
            In the second (bridging) join round, segments are joined if
            the gap between them is at most a fraction bridge_gap of their
            combined length (default: 0.5).
        """
        # Select rectangular region of simulation to be loaded
        box = self._make_selection_box()
        self._print(2, "Selection box is ["
                    "({:.3f} --> {:.3f}), "
                    "({:.3f} --> {:.3f}), "
                    "({:.3f} --> {:.3f})]"
                    .format(*box[0, :], *box[1, :], *box[2, :]))

        f = h5.File(map_file, 'r')

        # Identify cells intersecting the region of interest
        #     'cell_offsets' is the first cell per dimension
        #     'num_cells' is the number of cells per dimension
        cell_offsets, cell_lengths = self._identify_relevant_cells(box, f)
        self.num_cells = reduce(mul, cell_lengths, 1)       
        self._print(1, "Checking {:d} cells..." .format(self.num_cells))

        # No cells --> nothing to do --> done
        if self.num_cells == 0:
            self.num_segments = 0
            self.num_particles = 0
            self.files = np.zeros(0, dtype=int)
            return

        # Too many cells --> too much to do --> done
        if self.num_cells > 1e6:
            self.load_full = True
            return

        # The heavy lifting: find file index, offset, and length for
        # all potentially relevant segments
        self._find_segments(cell_offsets, cell_lengths, f)

        # Rule of thumb: close files...
        f.close()

        # If there are many segments, try to combine them where possible
        if self.num_segments > join_threshold:
            self._join_segments()

            # Second, more aggressive join: bridge small gaps
            # between segments
            if self.num_segments > bridge_threshold:
                self._join_segments(gap_factor=bridge_gap_factor)

        self.num_particles = np.sum(self.lengths)
        self.num_particles_exact = self.num_particles

        if self.exact:
            self._find_exact_region()

    def _find_exact_region(self):
        """Find particles lying exactly in the selection region."""
        anchor = self.coordinates[:3]

        # Need to explicitly set 'exact = False' here, because we have not
        # yet set up exact loading (we are doing it right now!).
        # Coordinates are in data units here, so need to be consistent.
        pt_coords = self.read_data("Coordinates", exact=False, units='data')
        rel_pos = pt_coords - anchor[None, :]

        if self.shape == 'sphere':
            rel_rad = np.linalg.norm(rel_pos, axis=1)
            self.ind_sel = np.nonzero(rel_rad <= self.coordinates[3])[0]

        elif self.shape == 'cube':
            if self.anchor_style == 'bottom':
                self.ind_sel = np.nonzero(
                    (np.min(rel_pos, axis=1) >= 0) &
                    (np.max(rel_pos, axis=1) <= self.coordinates[3]))[0]
            else:
                self.ind_sel = np.nonzero(
                    np.max(np.abs(rel_pos), axis=1) <= self.coordinates[3])[0]

        elif self.shape == 'box':
            length = self.coordinates[3:]
            if self.anchor_style == 'bottom':
                self.ind_sel = np.nonzero(
                    (np.min(rel_pos, axis=1) >= 0) &
                    (np.max(rel_pos - length[None, :]) <= 0))[0]
            else:
                self.ind_sel = np.nonzero(
                    np.max(np.abs(rel_pos) - length[None, :], axis=1) <= 0)[0]
        else:
            print("Invalid shape encountered: '{:s}'." .format(self.shape))
            set_trace()

        self.num_particles_exact = len(self.ind_sel)

        # Store coordinates, in correct units.
        self.Coordinates = (
            pt_coords[self.ind_sel, ...]
            * self.get_unit_conversion('Coordinates',
                                       self.read_units))

    def _make_selection_box(self):
        """
        Find the box enclosing the selection region.

        For the moment, *all* particles in this region will be loaded.
        In future, we may do something more fancy that takes the actual
        selection shape into account (`may' here includes `may not').

        This function returns a 3x2 array of 6 coordinates specifying the box:
        [[x-, y-, z-], [x+, y+, z+]]
        """
        coordinates = np.array(self.coordinates)
        box = np.zeros((3, 2))

        if self.shape == "sphere":
            if len(coordinates) != 4:
                print("A sphere needs four coordinates: "
                      "its centre (3), and radius (1)")
                set_trace()
            box[:, 0] = coordinates[:3] - coordinates[3]
            box[:, 1] = coordinates[:3] + coordinates[3]

        elif self.shape == "cube":
            if len(coordinates) != 4:
                print("A cube needs four coordinates: "
                      "its lower corner or centre (3) and "
                      "(half-)side-length (1)")
                set_trace()

            # Set up of lower box corner differs depending on anchor type
            box[:, 0] = coordinates[:3]
            box[:, 1] = coordinates[:3] + coordinates[3]
            if self.anchor_style == 'centre':
                box[:, 0] -= coordinates[3]

        elif self.shape == "box":
            if len(coordinates) != 6:
                print("A box needs six coordinates: its lower (3) and "
                      "upper (3) vertices, or centre (3) and half-side "
                      "lengths (3)")
                set_trace()

            if self.anchor_style == 'centre':
                box[:, 0] = coordinates[:3] - coordinates[3:]
                box[:, 1] = coordinates[:3] + coordinates[3:]
            elif self.anchor_style == 'bottom':
                box[:, 0] = coordinates[:3]
                box[:, 1] = coordinates[3:]
            else:
                print("Unrecognised anchor_style choice '{:s}'"
                      .format(self.anchor_style))
                set_trace()

        else:
            print("Your shape '{:s}' is not yet implemented."
                  .format(self.shape))
            set_trace()

        return box

    def _identify_relevant_cells(self, box, f):
        """
        Identify all cells intersecting the selection box.

        This function uses the lower corner of the region covered with cells
        and the cell size from the particle maps.

        Parameters
        ----------
        box : array [3, 2]
            The lower ([:, 0]) and upper ([:, 1]) vertices of the selection
            box as determined by _make_selection_box().
        f : h5py file handle
            The handle to the previously opened particle map file.

        Returns
        -------
        cell_offsets : array (int) [3]
            The first cell that intersects the selection box, per dimension.
        cell_lengths : array (int) [3]
            The number of cells intersecting the box, per dimension.
            In other words, in dimension i, cells cell_offsets[i] up to and
            including cell_offsets[i]+cell_lengths[i]-1 must be checked.

        Note
        ----
        The function checks whether the selection box extends beyond the edge
        of the mapped region, and clips the box to the edge if so.
        """
        # No particles --> no cells
        header = f["Header"]
        self.num_part_total = header.attrs["NumPart_Total"][self.pt_num]
        if self.num_part_total == 0:
            return [0, 0, 0], [0, 0, 0]

        # Get the key numbers from the particle map file
        ptGroup = f[self.base_group]
        cell_corner = ptGroup.attrs["CellRegionCorner"][:]
        cell_size = ptGroup.attrs["CellSize"][0]
        num_cells_per_dim = ptGroup.attrs["NumCellsPerDim"][:]
        cell_top = cell_corner + cell_size*num_cells_per_dim

        self._print(2, "cell_corner =", cell_corner)
        self._print(2, "cell_size =", cell_size)

        # Sanity check to make sure the box does not go below/above
        # the mapped region (also upper/lower box corner!)
        if np.min(box[:, 0] - cell_corner) < 0:
            self._print(
                1, "Warning: selection box extends below mapped region.\n"
                "Clipping it to edge of the map...")
            box[:, 0] = np.clip(box[:, 0], cell_corner, None)
            box[:, 1] = np.clip(box[:, 1], cell_corner, None)
        if np.min(cell_top - box[:, 1]) < 0:
            self._print(
                1, "Warning: selection box extends above mapped region.\n"
                "Clipping it to edge of the map...")
            box[:, 0] = np.clip(box[:, 0], None, cell_top)
            box[:, 1] = np.clip(box[:, 1], None, cell_top)

        # Find first cell to load in each dimension. Note that this is
        # never negative (we just made sure), so would not strictly have
        # to use np.floor here.
        cell_offsets = np.floor((box[:, 0]-cell_corner)/cell_size).astype(int)
        if np.min(cell_offsets) < 0:
            print("Why on Earth do we have negative cellOffsets???")
            set_trace()

        # Find number of cells to load in each dimension
        # (need to clip cellEnds to accommodate rounding errors --
        #  *not* needed with cellOffsets because box[:, 0] is clipped
        #  exactly to cellCorner)
        cell_ends = np.ceil((box[:, 1]-cell_corner) / cell_size).astype(int)
        cell_ends = np.clip(cell_ends, None, num_cells_per_dim)
        cell_lengths = cell_ends - cell_offsets

        self._print(2, "cell_offsets =", cell_offsets)
        self._print(2, "cell_lengths =", cell_lengths)
        return cell_offsets, cell_lengths

    def _find_segments(self, cell_offsets, cell_lengths, f):
        """
        Determine the 'segments' that have to be loaded.

        A segment is a cell section lying entirely in one file. Both the
        individual files and location within them are determined.

        Parameters
        ----------
        cell_offsets : array (int) [3]
            The first cell that intersects the selection box, per dimension.
        cell_lengths : array (int) [3]
            The number of cells intersecting the box, per dimension.
        f : h5py File object
            A handle to the previously opened map file.

        Stores in attribute
        -------------------
        num_segments : int
            The number of segments found in total.
        files : array (int) [num_segments]
            The file index in which each segment is located.
        offsets : array (int) [num_segments]
            The segments' first particle in the respective file.
        lengths : array (int) [num_segments]
            The number of particles in each segment.

        Note
        ----
        Currently, the file offsets are determined for all cells at the
        start of the function. This may be inefficient for small cell numbers.
        """
        # Load the map data
        ptGroup = f[self.base_group]
        map_cells_per_dim = ptGroup.attrs["NumCellsPerDim"]
        map_cells = ptGroup.attrs["NumCellsTot"][0]
        self._print(2, "Cells per dimension in map:", map_cells_per_dim)

        # To avoid confusion with the (overall) cellOffsets/-Counts,
        # call the 'particles-in-map-cells' counters 'mapOffsets/-Counts
        map_counts = _read_hdf5_direct(ptGroup, "CellCount", map_cells)
        map_offsets = _read_hdf5_direct(ptGroup, "CellOffset", map_cells)
        self.file_offsets = ptGroup["FileOffset"][:]
        self._print(2, "file_offsets=", self.file_offsets)
        map_cell_files = np.searchsorted(self.file_offsets, map_offsets,
                                         side='right')-1
        self.num_files = len(self.file_offsets)-1

        # Set up output arrays (large enough to account for worst-case of
        # each file gap dividing a selected cell)
        max_segments = self.num_cells + self.num_files
        self.files = np.zeros(max_segments, dtype=int)-1
        self.offsets = np.zeros(max_segments, dtype=int)-1
        self.lengths = np.zeros(max_segments, dtype=int)

        # Now loop through all possibly relevant cells...
        num_checked = 0
        self.num_segments = 0
        for cz in range(cell_offsets[2], cell_offsets[2]+cell_lengths[2]):
            for cy in range(cell_offsets[1], cell_offsets[1]+cell_lengths[1]):
                for cx in range(cell_offsets[0],
                                cell_offsets[0]+cell_lengths[0]):
                    if (num_checked+1 % 10000 == 0):
                        self._print(
                            1, "Checking cell {:d} (segments so far: {:d})"
                            .format(num_checked, self.num_segments))
                    num_checked += 1
                    index = _ind1d([cx, cy, cz], map_cells_per_dim, map_cells,
                                   periodic=self.periodic)
                    self._segmentise_cell(map_counts[index],
                                          map_cell_files[index],
                                          map_offsets[index])
                    if self.num_segments > max_segments:
                        print("Have somehow created too many segments...")
                        set_trace()

        # Final touches
        self.files = self.files[:self.num_segments]
        self.offsets = self.offsets[:self.num_segments]
        self.lengths = self.lengths[:self.num_segments]

        self._print(2, "Checked {:d} cells, found {:d} segments."
                    .format(num_checked, self.num_segments))

    def _segmentise_cell(self, cell_count, cell_file, cell_offset):
        """Analyse one cell and add its segments to the internal arrays.

        Parameters
        ----------
        cell_count : int
            The number of particles in current cell.
        cell_file : int
            The file index of the first particle in the cell.
        cell_offset : int
            The offset in the full, concatenated particle list of the
            first particle in this cell.
        """
        # Shortcut in case current cell is empty
        if cell_count == 0:
            return

        # Ok, if we get here there *are* some particles in the
        # current cell --> add to output list
        #
        # Need file index and file-offset of first cell particle
        offset_in_file = (cell_offset
                          - self.file_offsets[cell_file])
        if offset_in_file >= self.file_offsets[cell_file + 1]:
            print("Inconsistent file offset detected...")
            set_trace()

        first_elem = cell_offset
        last_elem = cell_offset + cell_count - 1

        # If this is a multi-file cell, truncate segment to end
        # of (current, first) file, and deal with it later
        if self.file_offsets[cell_file + 1] <= last_elem:
            length_in_file = (self.file_offsets[cell_file + 1]
                              - first_elem)
        else:
            length_in_file = cell_count

        # Save current segment in next free place of arrays
        self.files[self.num_segments] = cell_file
        self.offsets[self.num_segments] = offset_in_file
        self.lengths[self.num_segments] = length_in_file
        self.num_segments += 1

        # If needed, create additional segment(s) for following file(s)
        while self.file_offsets[cell_file + 1] <= last_elem:
            cell_file += 1

            # Number of elements depends on whether even now
            # the cell extends beyond the end of the file
            if last_elem >= self.file_offsets[cell_file + 1]:
                length_in_file = (self.file_offsets[cell_file + 1]
                                  - self.file_offsets[cell_file])
            else:
                length_in_file = (last_elem
                                  - self.file_offsets[cell_file] + 1)

            self.files[self.num_segments] = cell_file
            self.offsets[self.num_segments] = 0   # Always the case here!
            self.lengths[self.num_segments] = length_in_file
            self.num_segments += 1

    def _join_segments(self, gap_factor=None):
        """Combine adjoining or nearby segments to speed up reading.

        This function is called at the end of _setup_region(), after
        the segments have been created. It is purely a tuning option,
        updating the files, offsets, lengths, and num_segments attributes.

        Parameters
        ----------
        gap_factor : float or None, optional
            If None (default), only directly adjoining segments are joined.
            Otherwise, two consecutive segments are also joined if the
            ratio between their combined and bridged lengths is greater than
            <gap_factor>.
        """
        # Sort segments into sequential order
        sorter = np.lexsort((self.offsets, self.files))
        self.files = self.files[sorter]
        self.offsets = self.offsets[sorter]
        self.lengths = self.lengths[sorter]

        # Find segments that can be joined with the previous one:
        if gap_factor is None:
            # Segments that are directly adjacent to each other
            ind_join = np.nonzero(
                (self.offsets[1:] == self.offsets[:-1]+self.lengths[:-1])
                & (self.files[1:] == self.files[:-1]))[0] + 1
        else:
            # Segments separated by a small enough gap
            ind_join = np.nonzero(
                ((self.lengths[1:]+self.lengths[:-1])
                 / (self.offsets[1:]+self.lengths[1:] - self.offsets[:-1])
                 > gap_factor) & (self.files[1:] == self.files[:-1]))[0]+1

            # Need to 'back-extend' to-be-joined segments to the
            # end of the one they are joining (to add particles in gap)
            len_extra = (self.offsets[ind_join] - self.offsets[ind_join-1]
                         - self.lengths[ind_join-1])
            self.offsets[ind_join] -= len_extra
            self.lengths[ind_join] += len_extra
        num_joins = len(ind_join)
        self._print(2, "Identified {:d} joins for {:d} segments..."
                    .format(num_joins, self.num_segments))

        # Don't need to do anything if there's nothing to do
        if num_joins == 0:
            return

        ind_to_join = ind_join - 1
        if np.min(ind_to_join) < 0:
            print("Why oh why are we trying to join onto negative segment??")
            set_trace()

        # For efficiency, the actual joining is done in a hierarchical way.
        # In each step, 'odd in the i^th power' segments are joined:
        # 1. --> 1, 3, 5, 7, ... [standard odd numbers]
        # 2. --> 2, 6, 10, 14, ... [even, but not divisible by 4]
        # 3. --> 4, 12, 20, 28, ... [divisible by 4, but not 8]
        # ... and so forth.
        num_joined = 0
        jj = 0
        while True:
            jj += 1
            # Find segments eligible to join in this iteration
            self._print(2, "Join iteration {:d}..." .format(jj))
            subind_join_now = _find_odd_elements(ind_join, jj)

            # Join eligible segments onto their predecessors
            self.lengths[ind_to_join[subind_join_now]] += (
                self.lengths[ind_join[subind_join_now]])
            self.lengths[ind_join[subind_join_now]] = 0
            num_joined += len(subind_join_now)

            # Check whether we have joined everything that can be
            self._print(2, "Have so far completed {:d} joins ({:.2f}%)..."
                        .format(num_joined, num_joined/num_joins*100))
            if num_joined == num_joins:
                break

            if num_joined > num_joins:
                set_trace()

            # Update targets that have just joined for next iteration
            ind_target_joined = _find_odd_elements(ind_to_join, jj)
            targets_abs = np.arange(len(self.files), dtype=int)
            targets_abs[ind_join] = ind_to_join
            ind_to_join[ind_target_joined] = targets_abs[
                ind_to_join[ind_target_joined]]

        # Final step: discard all segments with non-zero length
        ind_remain = np.nonzero(self.lengths > 0)[0]
        self.num_segments = len(ind_remain)
        self.files = self.files[ind_remain]
        self.offsets = self.offsets[ind_remain]
        self.lengths = self.lengths[ind_remain]
        self._print(1, "Retained {:d} segments ({:.2f}%...)"
                    .format(self.num_segments,
                            self.num_segments/len(sorter)*100))

    def _get_dm_masses(self):
        """Construct an array of DM particle masses."""
        if self.base_group != 'PartType1':
            print("*** Cannot construct DM particle masses for this "
                  "ReadRegion type. ***")
            set_trace()
        if self.num_particles_exact is None:
            print("*** Cannot construct DM particle masses -- unknown "
                  "total number of particles. ***")
            set_trace()

        return np.zeros(self.num_particles_exact) + self.m_dm

# ----------- Below are short private helper functions ----------------------

def _ind1d(ind3d, num_3d, num_tot=None, periodic=False):
    """Convert a 3D to 1D index."""
    if num_tot is None:
        num_tot = num_3d[0]*num_3d[1]*num_3d[2]
    if periodic:
        for idim in range(3):
            ind3d[idim] = _periodic_wrap(ind3d[idim], num_3d[idim])

    ind1d = (ind3d[0] + ind3d[1]*num_3d[0] + ind3d[2]*num_3d[0]*num_3d[1])
    if ind1d < 0 or ind1d >= num_tot:
        print("Invalid 1D index generated ({:d}, total={:d})."
              "Please investigate."
              .format(ind1d, num_tot))
        set_trace()
    return ind1d


def _periodic_wrap(i, n):
    """Perform (single) periodic wrapping of index i to length n."""
    if i < 0:
        i += n
    elif i >= n:
        i -= n
    return i


def _find_odd_elements(index, power=1):
    """Find elements in index that are odd to the i^th power."""
    divisor = 2**power
    return np.nonzero(index % divisor == divisor/2)[0]


def _read_hdf5_direct(container, data_set, num, dtype=int):
    """Read 1D dataset into array [convenience function]."""
    array = np.zeros(num, dtype=dtype)
    container[data_set].read_direct(array)
    return array
